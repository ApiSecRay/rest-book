<chapter id="performance">
  <title>Performance and Scalability</title>
  <para>
    Performance and scalability are two different but related qualities (see 
    <xref linkend="non-functional-requirements"/>).
  </para>
  <para>
    <firstterm>Performance</firstterm> is the amount of useful work accomplished in a some period of time. 
    <firstterm>Scalability</firstterm> is the ability to handle a growing amount of work over time. 
    In other words, performance is about how long the system takes to perform a single activity,
    while scalability is about how that time changes as more and more work is performed by the system.
  </para>
  <para>
    There are two types of scalability: <firstterm>vertical scaling</firstterm> addresses the scalability of a single 
    instance of the service, while <firstterm>horizontal scaling</firstterm> focuses on multiple instances.
    Vertical scaling increases the work done by making the individual servers more powerful, while horizontal scaling 
    does the same by adding more servers, each of which doesn't have to be all that powerful.
  </para>
  <para>
    More powerful servers are relatively more expensive than less capable servers, so the economics are in favor of 
    horizontal scaling.
    Whenever we talk about scalability in this book, we always refer to the horizontal variant.
  </para>
  <para>
    Ideally, we want to keep the performance constant as the workload grows by simply adding more servers. This is 
    called <firstterm>linear scalability</firstterm>.
  </para>
  <para>
    Each server added to the cluster contributes processing power, but there is also some overhead of maintaining the
    cluster and coordinating work. The percentage of useful work that an additional server can deliver is called the 
    <firstterm>scalability factor</firstterm>. A scalability factor of 1 implies linear scalability.
  </para>
  <para>
    When the scalability factor is lower than 1.0, we have sub-linear scalability. This will usually be the case; the 
    trick is to get as close to linear scalability as possible. The difference between linear and sub-linear 
    scalability is shown in <xref linkend="scalability"/>.
  </para>
  <figure id="scalability">
    <mediaobject>
      <imageobject>
        <imagedata fileref="images/scalability.png" scale="68" align="center"/>
      </imageobject>
    </mediaobject>
    <title>Linear vs. sub-linear scalability</title>
  </figure>
  <para>
    Scalability is one of the driving forces behind the REST architectural style (see <xref linkend="introduction"/>). 
    This chapter offers a bag of tricks to help you achieve good performance and scalability.
  </para>
  
  <section id="stateless">
    <title>Statelessness</title>
    <para>
      We touched upon the stateless constraint in <xref linkend="rest"/>, where we saw that the information to 
      be maintained between two requests in an interaction between any one client and the (cluster of) server(s) is 
      divided up into two parts: application state and resource state.
    </para>
    <para>
      Resource state is the information that needs to be maintained for more than one client or for more than one 
      "session" of the same client. We use the term "session" very loosely here; it just means a collection of related 
      HTTP messages where the client is trying to achieve a single goal.
    </para>
    <para>
      Any information that is not resource state is application state. This is whatever information the client needs to 
      maintain while in pursuit of its goal. We'll discuss maintaining application state in clients in
      <xref linkend="application-state"/>.
    </para>
    <para>
      Some of the application state is only ever used by the client and remains local to it. The other part is required 
      for the server to process a request, which means the client must embed it in the message it sends to the server. 
    </para>
    <para>
      There is a trade-off here. Any state that the server must maintain will have to be persisted in some sort of 
      database. Moving information from resource to application state means that the server has less to persist and
      since storing information into a database and retrieving it from the database both take time, the overall 
      performance of the server increases when there is less information to persist.
    </para>
    <para>
      However, there is now more information transmitted in the messages between client and server. This has two 
      negative effects. The first is that there is more network traffic, so the server needs more I/O time to read a
      message. The second is that the server needs more CPU time to unpack the message.
    </para>
    <para>
      Generally speaking, this trade-off works well in favor of statelessness. By carefully implementing the I/O 
      handling, the server can reduce the impact of bigger messages. A growing trend in this area is reactive
      programming (see <xref linkend="reactive"/>).
    </para>
    <para>
      JavaScript handles JSON-based messages natively. Other combinations of media type and technology stack usually
      rely on specialized libraries to handle message serialization and deserialization. These libraries vary greatly 
      in speed, so do your homework well. Switching message serialization implementations may be an easy way to gain
      significant performance improvements. As with all performance optimizations, measure carefully across a range
      of realistic loads before you decide anything.
    </para>
    <para>
      Using less database connections or keeping them open for a shorter amount of time also helps with scalability.
      Database connections are scarce resources that are often pooled. The more requests need them, and the longer
      they are kept open, the bigger the chance that a new request will have to wait for a connection from the pool
      and the longer it will have to wait on average to acquire one if it has to wait. These problems become bigger 
      when more requests are being handled, which means we'll have sub-linear scalability.
    </para>
    <para>
      In RESTBucks, the customer composes an order from the items on the menu. The order in progress is application
      state rather than resource state, so it is maintained by the client. Only when the customer is done composing her 
      order, will it be transferred to the server.
    </para>
    <para>
      We could have designed the order-in-progress as resource state, as with many shopping carts on e-commerce 
      websites, but that would have been wasteful of server resources. The server doesn't need to do anything with the 
      order until it is complete. Other clients don't need to be aware of the order until it is finalized either. 
      Finally, the client doesn't need to maintain the order between sessions. What are the odds that you'll want to 
      start buying coffee today, then stop and come back to it tomorrow?
    </para>
    <para>
      By the way, the reason that these e-commerce websites store the shopping cart as resource state, is not because
      the people building these websites don't know what they're doing. Their clients are browsers, not the more 
      capable REST clients that we're envisioning consuming our API. Also, for some e-commerce scenarios it really does 
      make sense to start buying something and come back to complete the transaction later.
    </para>
  </section>
  
  <section id="granularity">
    <title>Granularity</title>
    <para>
      The granularity of the calls in an API can have a big impact on performance and scalability. We speak of a 
      <firstterm>fine-grained</firstterm> API when its calls are relatively small in terms of code size and execution 
      time. The opposite of a fine-grained API is a <firstterm>coarse-grained</firstterm> one. 
    </para>
    <para>
      Fine-grained APIs are more convenient to use, because they are very targeted and easy to understand. So from a 
      Developer eXperience (DX) perspective, you'll usually want to go for something more fine-grained.
    </para>
    <para>
      However, when API calls are smaller, a client needs to execute more of them to achieve the same goal. This means 
      more network latency and more overhead in the server to handle all those requests. So from a performance and
      scalability perspective, you'll want to go for something more coarse-grained.
    </para>
    <para>
      Where to draw the line is hard to say in general, because it really depends on the specifics of the API and the
      goals that clients are trying to achieve. Since premature optimization is the root of all evil 
      <citation>Knuth74</citation>, we advise to start with good DX and only make the API more coarse-grained when
      and where needed.
    </para>
    <para>
      The granularity of network calls is distinct from but related to the granularity of data. The relationship is
      easy to see when we consider data transfer objects.
    </para>
    <para>
      A <firstterm>Data Transfer Object</firstterm> (DTO) is defined as <quote>an object that carries data between 
      processes in order to reduce the number of method calls</quote> <citation>Fowler12</citation>.
      If this sounds familiar, then that's because that's exactly what self-describing messages in RESTful systems are.
    </para>
    <para>
      This means that all the best practices around DTOs apply to REST messages too. One example is to 
      <quote>encapsulate the serialization mechanism for transferring data over the wire</quote>. We'll discuss how to
      do that in <xref linkend="dry"/>.
    </para>
    <para>
      DTOs are very different from domain objects. <firstterm>Domain objects</firstterm> implement the ubiquitous 
      language used by subject matter experts and thus are discovered <citation>Evans04</citation>. DTOs, on the other 
      hand, are designed to meet certain non-functional characteristics, like performance, and thus are subject to all 
      kinds of design trade-offs.
    </para>
    <para>
      You may be tempted to expose your domain objects directly as messages, especially when you're using a technology
      stack that makes this easy, like Spring Data REST <citation>SpringDataRest</citation>. While this may work,
      it can easily lead to poor performance, because domain objects are often very granular.
    </para>
    <para>
      There is nothing wrong with starting out with domain objects for your messages, as long as you remember that they
      are not the same. Don't change your domain objects to reduce the number of HTTP messages exchanged, but map 
      several domain objects onto a single message to get better performance.
    </para>
    <para>
      If you start with the functionality rather than the data, as we discussed in <xref linkend="state-diagrams"/>, 
      you're much less likely to walk into the domain-objects-as-messages trap.
    </para>
    <para>
      So how do you make your messages more coarse-grained? 
    </para>
    <para>
      One approach is to take your state diagrams and look for adjacent transitions that can be combined. In the case 
      of <xref linkend="sd-customer-happy-path"/>, for instance, you could combine the <literal>Place order</literal> 
      and <literal>Pay</literal> transitions. Or <literal>Pay</literal> and <literal>Take receipt</literal>. Or all
      three of them.
    </para>
    <para>
      As with any performance optimization, you should measure the effect of the change. Only keep a change if it
      significantly improves performance, because you're often making the API less convenient to use when you combine
      transitions. Remember that the state diagram captures real usage scenarios (see <xref linkend="bdd-to-sd"/>).
    </para>
    <para>
      Think carefully about how you've changed the interactions. If you combine <literal>Place order</literal> and
      <literal>Pay</literal>, for example, you're taking away the ability to change the order or to cancel it. In this
      case, that's probably okay, because both operations can still be achieved. Changing the order has to be done
      before it is sent and canceling it means not sending any order at all. This example thus works very well. We
      would still propose that you only make this change after you've established <emphasis>through 
      measurements</emphasis> that it is going to improve performance <emphasis>in realistic scenarios</emphasis>.
    </para>
    <para>
      Another approach to arrive at a more coarse-grained API is to completely rethink the interaction model that you
      captured in the state diagram. This isn't always possible to do, since you still have to meet the requirements.
      But sometimes it helps to think about the requirements differently.
    </para>
    <para>
      For example, some of the messages exchanged in RESTBuck deal with payments. We could envision a 
      different model where customers pay a larger sum up front and their orders are deducted from their balance. 
      Or maybe customers could pay after the fact, when we send them an invoice for their recent orders. Both approaches
      reduce the number of messages exchanged, because a single payment message now covers a number of orders.
    </para>
    <para>
      As these examples show, rethinking your interaction usually means rethinking your business. Needless to say, that
      has a big impact and may not be an option. But if it is, it can open up big opportunities for performance
      improvements. Sometimes such an opportunity may even have the fortunate side effect of improving the Developer 
      eXperience as well.
    </para>
    <para>
      There is a third approach that works especially well with collection resources (see <xref linkend="collection"/>).
      It is called <firstterm>inlining</firstterm>, which means embedding representations of the collection's items 
      inside the representation of the collection itself.
    </para>
    <para>
      Inlining may or may not be a good idea. For large collections, you probably want to reduce the amount of bytes
      transferred rather than increase it (see <xref linkend="paging"/>). For smaller collections, it may make sense
      to inline the items to save some network roundtrips. This is especially the case if the client can filter the 
      items in the collection so it only returns interesting items.
    </para>
    <para>
      Since it's often hard to predict whether inlining is a good idea, it's usually best to leave the choice up to the 
      client. It can use a URI query parameter to indicate whether it wants the server to inline the collection items.
      Use a sensible default value if you can.
    </para>
    <para>
      If your API has multiple collections, which most APIs do, then pick a
      default that works for the majority and use that as the default for all of them. Consistency is an important
      aspect of the Developer eXperience. If you have trouble deciding the default value, go with 
      <literal>false</literal> to be on the safe side.
    </para>
    <para>
      We can go a step further and generalize the concept of inlining beyond collections. If two resources are related, 
      we can embed a representation of one into the representation of the other so that the client doesn't have to 
      spend a network roundtrip to follow the link between the resources.
    </para>
    <para>
      For instance, consider an API that maintains persons and their addresses. Each person may have multiple addresses 
      and each address may house multiple people. Normally we would make the addresses of a person a separate 
      collection resource, but we can also include all of them in the representation of the person. Clients requesting
      information about a person may often want to know their addresses, so this may save some network roundtrips.
    </para>
    <para>
      We can take this technique too far as well. Imagine a representation of a person that embeds the representation 
      of an address that embeds the representation of a country that embeds representations of all its states that ...
      Before you know it the server spends a lot of time querying the database for data that the client will never
      need and in doing so waste more time transmitting all that data than it saves in network roundtrips.
    </para>
    <para>
      For collections the concept of inlining is simple and is usually indicated by the <literal>inline</literal> URI 
      query parameter.
      There isn't such a standard for the general inline case. A boolean parameter may not make sense if the inlining 
      could be multiple levels deep. A <literal>depth</literal> parameter could be the solution in that case.
    </para>
    <para>
      If you're making it possible to inline multiple levels, then you risk running into infinite recursion. For 
      example, the representation of a person may include the representation of its spouse, which in turn may include 
      the representation of its spouse (which is the original person), which ...
    </para>
    <para>
      It's up to the server to detect such cyclic relationships and take corrective action, as we'll discuss in 
      <xref linkend="input-validation"/>.
      You don't want to leave that up to the client because it may accidentally forget that or it may be an attacker 
      doing it on purpose in an attempt to bring your system down. (We discuss malicious usage in 
      <xref linkend="threat-modeling"/>.)
    </para>
    <para>
      Even when infinite recursion is not possible, you may still be at risk. In a large data model with a high level of 
      connectedness, a large value of <literal>depth</literal> may drain your server of resources and be used as a
      denial of service attack (see <xref linkend="security"/>). Again, use input validation to prevent such attacks.
    </para>
  </section>
  
  <section>
    <title>Return Created Or Updated Representations</title>
    <para>
      In many cases, a client will create a resource and then continues to work on its representation. It may create
      sub-resources, for instance, or follow link relations. This is especially common in the Workflow pattern (see 
      <xref linkend="workflow"/>).
    </para>
    <para>
      In such cases we can easily save the client a network roundtrip by including the representation of the created
      resource in the response with the <literal>201 Created</literal> status. You should return exactly the
      representation that the client would receive in response to a <literal>GET</literal> on the URI in the
      <literal>Location</literal> header, including a link with the <literal>self</literal> link relation that contains
      the location of the created resource.
    </para>
    <para>
      This technique is also applicable when a resource is being updated using the <literal>PUT</literal> or 
      <literal>PATCH</literal> methods. You might think that a client wouldn't <literal>GET</literal> what it just 
      <literal>PUT</literal>, but there are definitely cases where that is to be expected. For instance, the server may
      respond to updating a status property by adding a link that wasn't available before and the client may want to
      follow the link. This is one example where the server adds information to the resource beyond what the client
      provides, but there are others. This added information may be a reason for the client to request the created or
      updated resource.
    </para>
    <para>
      If you have doubts whether this technique is a good idea in your specific situation, look at your logs (see 
      <xref linkend="logging"/>) to determine how often <literal>PUT</literal>s or <literal>PATCH</literal>es are 
      followed by <literal>GET</literal>s. This requires that you can correlate requests by sender, which you can 
      usually do by IP address. Be careful when using load balancers, however, since they may hide the client's address.
    </para>
    <para>
      As usual, there is a trade-off. Using this technique will save network roundtrips at the expense of larger 
      response messages. It will take the server a little longer to compose the larger messages and it will take the 
      messages a little longer to travel the network. These costs are almost always significantly lower than the gain 
      of having to process fewer requests, making this technique one of the few widely applicable low hanging fruits.
    </para>
    <para>
      We've already seen this technique in action for RESTBucks in <xref linkend="http-happy-path"/>. The client places
      an order using <literal>POST</literal> and the server returns a representation of the order, including an 
      operation to pay for it. The client can execute this operation immediately, without first having to 
      <literal>GET</literal> a representation of the order.
    </para>
  </section>
  
  <section id="paging">
    <title>Paging &amp; Filtering</title>
    <para>
      Collections are fundamental to almost every API, as we saw in <xref linkend="collection"/>. Some collections
      will remain modest in size, but other collections can become very large. What will a client requesting 
      such a large collection do with the result? There are a few cases where a client really wants to plough through
      a million (or billion!) items, but such cases are rare, especially when the items are presented to a human user.
    </para>
    <para>
      Two scenarios are likely. The first is that the client is looking for one specific item in the collection. It
      can identify this item by name or some other property and it will go through the returned items looking for this
      special item. For instance, a RESTBucks customer may go through the items on the menu looking for the latte that
      the customer is craving.
    </para>
    <para>
      The other likely scenario is that the client doesn't quite know what item it is looking for, but it knows a 
      couple of things that must be true for an item to be interesting. For example, a RESTBucks customer may
      browse the menu looking for a hot beverage that does not contain caffeine without knowing beforehand what items 
      match those criteria.
    </para>
    <para>
      Both scenarios have in common that the number of items that are of interest to the client is small compared to
      the total number of items. Therefore, returning all of the items is wasteful and should be avoided.
    </para>
    <para>
      There are two ways to limit the number of items returned. The first is to blindly return some items
      but not others; the second is to return only items that are interesting to the client. The first option is
      called <emphasis>paging</emphasis>; the second is <emphasis>filtering</emphasis>.
    </para>
    <para>
      A paged collection serves its members in chunks of a particular size called <firstterm>pages</firstterm>. 
      Paged collections accept URI query parameters that indicate the starting page number and the number of items per 
      page that the client wants to receive.
      The representation contains links to the first, last, previous and next pages, if applicable, as shown in
      <xref linkend="paged-collection"/>.
      The client then navigates the pages in search for the item(s) it is interested in.
    </para>
    <figure id="paged-collection">
      <mediaobject>
        <imageobject>
          <imagedata fileref="images/paging.png" scale="50" align="center"/>
        </imageobject>
      </mediaobject>
      <title>A paged collection</title>
    </figure>
    <para>
      If the items in the collection come from a database, it's best to implement paging using support from that 
      database. For instance, you could modify your
      query to the database such that it returns the correct items. If you can't do that, you may be forced to have the
      database return all items, only to discard most of them when building the response. Not only would that loose 
      much of the potential performance gain, it could have disastrous effects on scalability.
      Again, it's imperative that you measure the impact of your implementation.  
    </para>
    <para>
      Paging can be useful in situations where the client doesn't really know what it's looking for, like when it simply
      wants to present some items to a human user. There are usually alternative and better designs where that isn't
      needed, however. This is one of those rare occasions where usablitity and performance may actually go hand in 
      hand, so if that's the case then jump on that unique opportunity.
    </para>
    <para>
      Note that if a client wants to process all items, then it actually hurts performance to break up the collection
      in pages, because it now requires more messages to transmit all items and extra links to the other pages.
      Again, think through your use cases and measure carefully.
    </para>
    <para>
      Paging is pretty dumb in the sense that the server has no clue which of the items transmitted are interesting to
      the client. That means that the chances are pretty good that time and bandwidth are wasted transmitting items that
      are not particularly interesting to the client. The exception is when the collection is ordered and the earlier
      items are more interesting than the latter ones (or vice versa). 
    </para>
    <para>
      A good example of this is the timeline that many social networks present: older messages are usually less 
      interesting than newer ones, so the most recent ones are presented first and the user may scroll as far back in 
      time as desired. This is an excellent case for paging.
    </para>
    <para>
      In most case, however, filtering is a better solution than paging, because it allows the client to tell the 
      server what items it
      is interested in, preventing the server from having to transmit uninteresting items. A filter is a search 
      condition that limits which collection items are returned. The search condition is usually expressed using URI 
      query parameters, for instance using those defined by the OpenSearch standard <citation>OpenSearch</citation>.
    </para>
    <para>
      Filter expressions range from simple name/value pairs to full-blown search languages. Name/value pairs are
      often convenient to select a few items from a large set, as SQL has taught us. They are easy to capture in URI
      query parameters, and easy to translate into database queries.
    </para>
    <para>
      We've already seen an example of a filter in <xref linkend="lifecycle"/>, where a RESTBucks barista client 
      queries the server for paid orders using the filter expression <literal>status=paid</literal>. It could have
      added paging on top of that to make sure it would only ever get one order back.
    </para>
    <para>
      Inventing your own search language is a big undertaking (although you may be able to start small) and you'll have 
      to ask yourself whether it is worth the trouble. A search language requires a parser for that language. If you
      start small and have to change the language, you may be forced to support more than one version of the search
      language, which could make the parser more complicated. You can prevent that by specifying the search query in 
      the entity-body rather than in a query parameter and have the client use content negotiation to indicate 
      which version of the search language it speaks. We'll talk more about content negotiation as a means to support
      evolution of APIs in <xref linkend="breaking-changes"/>.
    </para>
    <para>
      Another disadvantage of a search language is that it couples the client to the server implementation beyond its 
      REST API. In effect, the language extends the API. If you're not careful, you may end up building a new API that 
      doesn't benefit from the advantages of the REST architectural style.
    </para>
    <para>
      If the number of combinations of things to search for is limited, you may forego a filter and offer a specialized
      resources instead. For instance, in RESTBucks we could have introduced a collection resource that returns paid
      orders, rather than using a filter.
    </para>
    <para>
      If, in spite of these warnings, you do decide to design a full-featured search language, look at several existing 
      languages first <citation>XPath</citation> <citation>JSONPath</citation> <citation>OCL</citation>. You may be able 
      to re-use all or parts of existing search languages. This means less design work for you (and potentially also 
      less implementation work if you can re-use libraries), but also less things to learn for the consumers of your 
      search language.
    </para>
    <para>
      For collections or other resources that support inlining (see <xref linkend="granularity"/>), you need to decide
      whether your filter can look at the inlined resources and, if so, how. be careful, because there is a possibility 
      that you'll end up making you filter so complex that you are in fact inventing a small search language.
    </para>
    <para>
      We can take the ideas of inlining and filtering to their logical conclusion. Imagine a collection that inlines 
      all its items, and the items can inline their relationships, which can inline their relationships, etc. 
      Further imagine that we have a filter that can look deeply into the inlined resources.
      This is the basic idea behind faceted search.
    </para>
    <para>
      <firstterm>Faceted search</firstterm> is a technique for accessing information organized according to a faceted 
      classification system, allowing users to explore a collection of information by applying multiple filters
      <citation>WikiFaceted</citation>.
      Facets allow clients to zoom in on the information they want in a very natural way. Human users love it because
      it is so easy to use. As you can imagine, however, it requires lots of server resources and may hurt both
      performance and scalability, unless you have an efficient way to calculate the facets.
    </para>
  </section>
  
  <section id="caching">
    <title>Caching</title>
    <para>
      Having a coarser-grained API means sending less messages over the network to get the same work done. 
      This is one example of doing less work to improve performance. Another example is to cache information.
    </para>
    <para>
      A <firstterm>cache</firstterm> is a component that stores data so future requests for that data can be served 
      faster. The data stored in a cache may be the result of an earlier computation, or a duplicate of data 
      stored elsewhere <citation>WikiCache</citation>.
      Web browsers use caches and so may REST clients. Many HTTP libraries offer such functionality out of the box.
    </para>
    <para>
      If the client doesn't have a representation in its cache, it has no choice but to send a normal request. This is 
      called a <firstterm>cache miss</firstterm>, as opposed to a <firstterm>cache hit</firstterm> when the data is
      found in the cache. The percentage of accesses that result in cache hits is known as the <firstterm>hit 
      rate</firstterm> or <firstterm>hit ratio</firstterm> of the cache.
      You can improve that ratio by adding a caching proxy.
    </para>
    <para>
      A <firstterm>proxy</firstterm> is a program that sits between client and server. In other words, it acts as a 
      server to the client and as a client to the server. Remember from <xref linkend="rest"/> that the layered system 
      constraint allows us to insert a proxy between client and server without either even noticing. To distinguish 
      between the proxy and the actual server, the latter is referred to as the <firstterm>origin server</firstterm>.
    </para>
    <para>
      A <firstterm>caching proxy</firstterm> is a proxy that maintains a large cache of the responses that the server 
      returns. Because a caching proxy serves many clients, it can have a better cache hit ratio than any single client
      alone ever could.
    </para>
    <para>
      A caching proxy is usually part of the server landscape and clients are seldomly aware of it. It often makes 
      economic sense for the organization that runs the servers to install caching proxies, because it reduces the load 
      on the servers that do the actual work. This means less database operations, for example, which improves 
      scalability. Better scalability means the company needs to employ fewer servers to handle the same traffic.
    </para>
    <para>
      An alternative for deploying your own caching proxies is to use a <firstterm>Content Distribution
      Network</firstterm> (CDN, also known as <firstterm>Content Delivery Network</firstterm>).
      The goal of a CDN is to serve content to end-users with high availability and high performance.
      A CDN provides a cluster of caching proxies in various locations and they are often smart enough to serve content 
      for one that is geographically close to the client. In addition to caching, this has the advantage that it takes 
      less time for the messages to traverse the network. This doesn't help with performance or scalability on the 
      server, but it does improve the <firstterm>perceived performance</firstterm> on the client.
    </para>
    <para>
      A common problem with caches is that they may go <firstterm>stale</firstterm> when the information they contain 
      is no longer up-to-date. A client using data from a stale cache may draw the wrong conclusions and take 
      inappropriate actions. In some cases this may be perfectly acceptible, but in other cases it may mean the 
      difference between life and death, so do your homework and find out which situation you're in.
    </para>
    <para>
      The opposite of stale data is <firstterm>fresh</firstterm> data. 
      The server has two mechanisms to indicate how long a response stays fresh.
    </para>
    <para>
      The first mechanism is the <literal>Expires</literal> header defined by RFC 7234 <citation>HTTP</citation>.
      It tells all caches how long the associated representation is fresh for. A fresh representation may be returned 
      from the cache immediately, avoiding any request to the origin server.
    </para>
    <para>
      Different resources will have vastly different values for the <literal>Expires</literal> header.
      A Home resource (see <xref linkend="home"/>) will probably be good for quite some time, but other resources may 
      not. For instance, if your API serves stock traders, it's probably not a good idea to cache stock prices for too
      long.
    </para>
    <para>
      The value of the <literal>Expires</literal> header is an HTTP date, which means the precision is one second. Note
      that an HTTP date is in Greenwich Mean Time (GMT), not local time. The problem with using dates, of course, is 
      that client and server must share the same clock, which is not at all guaranteed unless they're known to use the
      same time server. It's therefore better to use the <literal>Cache-Control</literal> header.
    </para>
    <para>
      The <literal>Cache-Control</literal> header supports several parameters. The <literal>max-age</literal> parameter
      is the second mechanism to report the freshness interval. It is similar to the <literal>Expires</literal> header 
      in that it controls the freshness of the response, but it doesn't suffer from the clock synchronization issue, 
      because the value of this parameter is the number of seconds the representation stays fresh, rather than a date.
    </para>
    <para>
      <literal>Cache-Control</literal> takes precedence over <literal>Expires</literal>. To avoid confusion, we advise
      to use only <literal>Cache-Control</literal>. See <xref linkend="http-happy-path"/> for an example of
      <literal>Cache-Control</literal> in RESTBucks.
    </para>
    <para>
      The <literal>no-cache</literal> parameter of the <literal>Cache-Control</literal> header means the response is 
      never fresh. This is equivalent to a <literal>max-age</literal> with the value 0.
    </para>
    <para>
      If a response is found in the cache that is outside the freshness interval, then the response may be either fresh
      or stale. The client can't know which is the case, so it has two options. It can either send a normal request, 
      just as it would do in case of a cache miss, or it can try to determine whether the cache is in fact still fresh.
    </para>
    <para>
	    When we discussed concurrency in <xref linkend="concurrency"/>, we saw how to use the <literal>ETag</literal>
	    / <literal>If-Match</literal> and <literal>Date</literal> / <literal>If-Unmodified-Since</literal> header 
	    combinations to avoid overwriting someone else's changes.
	    We can use the same system to check the freshness of cached data, as shown in <xref linkend="caching-proxy"/>.
    </para>
    <figure id="caching-proxy">
      <mediaobject>
        <imageobject>
          <imagedata fileref="images/caching-proxy.png" scale="68" align="center"/>
        </imageobject>
      </mediaobject>
      <title>A caching proxy</title>
    </figure>
    <para>
      The client sends a <firstterm>conditional <literal>GET</literal></firstterm> request by adding the 
      <literal>If-Match</literal> or <literal>If-Unmodified-Since</literal> header. Conditional <literal>GET</literal>s are 
      covered by RFC 7232 <citation>HTTP</citation>.
      If the resource is still fresh, the server responds to a conditional <literal>GET</literal> with a
      <literal>304 Not Modified</literal> status <emphasis>without the entity-body</emphasis>. The client then
      retrieves the representation from its cache instead.
    </para>
    <para>
      If, on the other hand, the resource has changed compared to the value of the <literal>If-Match</literal> or 
      <literal>If-Unmodified-Since</literal> headers, the server responds with the normal status code and includes the
      new representation in the entity-body of the message. The client then updates its cache with the new 
      representation.
    </para>
    <para>
      Conditional <literal>GET</literal>s ensure that no stale data is ever used, at the expense of some additional, 
      but small, messages. Whether this is a good trade-off depends on the cache hit ratio. As we've seen, we can 
      improve this ratio using caching proxies. 
    </para>
    <para>
      Unfortunately, not all messages can be cached. Caching only works for information retrieval, not for updates.
      In other words, caching is limited to <literal>GET</literal> requests. But not all <literal>GET</literal>
      requests can be cached either, for various reasons:
    </para>
    <orderedlist>
      <listitem>
		    <para>
		      The server may not return a representation of the resource, for instance when it returns the <literal>202 
		      Accepted</literal> status code.
		      So a client should inspect the status code to determine whether it can cache the representation.
		      Responses with the following status codes are cacheable by default: 
		      200, 203, 204, 206, 300, 301, 404, 405, 410, 414, and 501. Other status codes are only cacheable if the 
		      server indicates so using the <literal>Cache-Control</literal> header.
		    </para>
      </listitem>
      <listitem>
		    <para>
		      The request may be to a protected resource (see <xref linkend="authentication"/>). One client may have
		      access to the resource, but another not. So the resource must either be cached specifically for the requesting
		      user, or not at all. With a large number of users, caching per user may not be practical, as the cache hit 
		      ratio becomes too low (or the amount of memory needed for the cache too high).
		    </para>
		    <para>
		      The <literal>private</literal> parameter of the <literal>Cache-Control</literal> header allows caching only 
		      for caches that are specific to one user (e.g. on the client, but not in a caching proxy).
		      The <literal>public</literal> parameter marks authenticated responses as cacheable. By default, any resource 
		      protected by authentication is not cacheable, so you should only make resources protected that absolutely 
		      require it. When you do so, combine the <literal>public</literal> and <literal>no-cache</literal> parameters
		      to allow caching with conditional <literal>GET</literal>s. This doesn't save any network roundtrips, because 
		      there must always be an up-to-date check, but it does benefit from smaller messages being transmitted in case 
		      the cache is still fresh.
		    </para>
      </listitem>
      <listitem>
		    <para>
		      If the request is secured using HTTPS (see <xref linkend="tls"/>), it won't be cached either.
		      This is a major problem for REST APIs, that often must use HTTPS to protect sensitive content.
		      You can solve this problem by terminating the HTTPS connection at the load balancer, and then have the load 
		      balancer route the traffic to a cluster of caching proxies rather than the origin servers. We discuss
		      load balancers in <xref linkend="load-balancing"/>.
		      The downside of this approach is that the traffic is now unprotected within the provider's network and thus 
		      accessible to insiders. We'll talk more about insider threats in <xref linkend="threat-modeling"/>.
		    </para>
      </listitem>
      <listitem>
		    <para>
		      If there is no validator (<literal>ETag</literal> or <literal>Last-Modified</literal>) present in the
		      response, then it is also considered uncacheable. We strongly advise to return one of these headers with all
		      responses for both caching and concurrency control purposes.
		    </para>
      </listitem>
      <listitem>
		    <para>
		      Some resources fluctuate so much, that caching is just a waste of memory.
		      The server can explicitly state that its response should not be cached using the 
		      <literal>Cache-Control</literal> header and the <literal>no-store</literal> parameter.
		      The <literal>Pragma: no-cache</literal> header value is an older and more limited alternative to achieve the
		      same. It is not as widely supported, however, so we advise against its use.
		    </para>
      </listitem>
    </orderedlist>
  </section>
  
  <section id="ranges">
    <title>Content Ranges</title>
    <para>
      Some messages returned by the server may be very large, for instance when the client requests a previously
      uploaded file or a generated report. This has its own set of performance and scalability issues.
    </para>
    <para>
      Transmitting large messages means holding a TCP connection open for a long time. If the server uses threads from a
      thread pool to handle messages, this means that one thread stays busy for a long time. This in turn means that
      a new request may block while waiting for a thread to become available for processing it. This is a shame,
      because the thread may not be doing much other than waiting for I/O.
    </para>
    <para>
      This realization led people to devise other ways of handling requests than using a thread per request. The
      difference in performance between the Nginx and Apache web servers, for example, is a clear indicator that there 
      is much to be gained from pursuing such alternatives. The Node.js server is another good example. We'll discuss
      these more modern approaches in <xref linkend="reactive"/>.
    </para>
    <para>
      Even with reactive programming, though, there are downsides to serving large messages. The number of TCP 
      connections that may be simultanously open in your infrastructure may be limited as well, for instance. Or your
      clients may be on mobile devices with spotty network connectivity. Large messages are more likely to fail in such
      conditions. To add insult to injury, the messages would have to be re-transmitted until fully received. 
    </para>
    <para>
      For all these reasons, it is better to have smaller messages. We would, therefore, like to split up large messages
      into chunks. We can do that using content ranges. With this technique, the client indicates the range of bytes 
      from the content that it wants to receive, hence the name.
    </para>
    <para>
      RFC 7233 defines several headers for this purpose <citation>HTTP</citation>:
    </para>
    <variablelist>
      <varlistentry>
        <term><literal>Accept-Ranges</literal></term>
        <listitem>
          <para>
            This response header field allows a server to indicate that it supports range requests for the target 
            resource. The value of the header is either <literal>none</literal> to indicate that range requests are not 
            allowed, or a unit of ranges. The only unit defined by the HTTP specification is <literal>bytes</literal>.
          </para>
          <para>
            A client may issue a range request without having seen an <literal>Accept-Ranges</literal> header in the
            response (or else we'd have a chicken and egg problem).
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term><literal>Range</literal></term>
        <listitem>
          <para>
            This request header field on modifies the semantics of the <literal>GET</literal> method to request 
            transfer of only one or more subranges of the selected representation data, rather than the entire selected
            representation data. The value consists of one or more ranges of bytes.
          </para>
          <para>
            A byte range consists of two positions, separated by a dash (<literal>-</literal>), e.g. 
            <literal>0-499</literal> to receive the first 500 bytes. A client may also leave out one of the two 
            positions, e.g. <literal>-500</literal> for the last 500 bytes, or <literal>500-</literal> for the 501th
            and following bytes.
          </para>
          <para>
            The <literal>Range</literal> header value may consist of more than one byte range separated by commas,
             e.g. <literal>0-0,-1</literal> for the first and last bytes.
          </para>
          <para>
            A server must ignore range requests for any method other than <literal>GET</literal>. Even for 
            <literal>GET</literal> requests, it may ignore them and send the full representation. If the request is
            conditional, and it would result in <literal>304 Not Modified</literal>, the server must also ignore the
            range aspect of the request.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term><literal>If-Range</literal></term>
        <listitem>
          <para>
            This request header field can be used to make a <firstterm>conditional range request</firstterm>. If the 
            representation is unchanged, the requested ranges are returned. If, however, the representation has 
            changed, the entire representation is returned.
          </para>
          <para>
	          The value of this header is either an ETag or an HTTP date, as explained in <xref linkend="caching"/>.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term><literal>Content-Range</literal></term>
        <listitem>
          <para>
            This response header field describes what range of the selected representation is enclosed. This may be
            different from the requested range, for instance when the client left out the starting or ending position,
            or when the client requested more bytes than the representation contains.
          </para>
          <para>
            The value of this header is the range unit followed by the range, followed by the total length, 
            e.g. <literal>bytes 0-500/1234</literal>. If the total length is unknown or costly to determine, the server
            may send an asterisk (<literal>*</literal>) instead, e.g. <literal>bytes 0-500/*</literal>.
          </para>
          <para>
            This response header field must only be used if the server returns a single range. For multiple ranges,
            the server must use the <literal>multipart/byteranges</literal> media type, which is another variation of 
            the multipart media types we saw in <xref linkend="batch"/>. Each body part will contain the 
            <literal>Content-Range</literal> header with the range for that part.
          </para>
        </listitem>
      </varlistentry>
    </variablelist>
    <para>
      If a server honors a range request, it must send the <literal>206 Partial Content</literal> status code 
      rather than <literal>200 OK</literal>. When multiple ranges are requested, a server may coalesce any of the
      ranges that overlap, or that are separated by a gap that is smaller than the overhead of sending multiple 
      parts (typically around 80 bytes).
    </para>
    <para>
      If the <literal>Range</literal> header field contains an invalid range, e.g. where the starting position is
      greater than the ending position, then the server should return <literal>416 Range Not Satisfiable</literal>. It
      should also return this status if none of the requested ranges overlap the current representation's length. A
      server may return <literal>200 OK</literal> with the complete representation instead. 
    </para>
    <para>
      Whenever a <literal>416 Range Not Satisfiable</literal> status is returned, the server should also include the
      <literal>Content-Range</literal> header with the actual representation length. 
      For instance, if the representation is 500 bytes, then a request with <literal>Range: 600-900</literal> should 
      get a <literal>416</literal> response with <literal>Content-Range: bytes */500</literal>.
    </para>
    <para>
      We haven't used content ranges in RESTBucks, because all the messages that we've seen so far were fairly small.
      One place where we could apply content ranges would be for images of the items on the menu.
    </para>
    <para>
      Unfortunately, some caches don't store content ranges. Test your environment carefully, or you risk undoing
      one performance optimization by introducing another.
    </para>
  </section>
  
  <section id="load-balancing">
    <title>Load Balancing</title>
    <para>
      You will most likely want to employ more than one server in a cluster, since a single server can't handle a 
      growing amount of traffic and when it fails your entire service is down.
    </para>
    <para>
      As soon as you have more than one server, a choice must be made which one handles an incoming request. You don't
      want to leave that choice to the client, because that would be coupling the client to the server farm.
    </para>
    <para>
      Instead, you will want to deploy a <firstterm>load balancer</firstterm>, whose job it is to optimize resource 
      use, maximize throughput, minimize response time, and avoid overload of any single resource.
      Load balancers can be implemented in hardware or in software.
    </para>
    <para>
      Load balancers are usually placed in a demilitarized zone (DMZ) a physical or logical subnetwork that contains 
      and exposes an organization's external-facing services to a larger and untrusted network like the Internet. DMZs
      improve security by shielding the server cluster from the outside world.
      Load balancers are often the terminating point of an TLS connection, as we'll see in <xref linkend="tls"/>.
    </para>
    <para>
      Load balancers should themselves be clustered to avoid any single point of failure. Since load balancers have 
      less work to do than the origin servers, a cluster of size two is usually sufficient.
    </para>
    <para>
      Load balancers can improve performance by reducing the amount of traffic through compression. A load balancer
      can also employ <firstterm>TCP buffering</firstterm>, where it buffers responses from the server and spoon-feeds 
      the data out to slow clients, allowing the web server to free a thread for other tasks faster than it would if it 
      had to send the entire request to the client directly.
    </para>
    <para>
      The algorithm that a load balancer uses may impact performance and scalability. The simplest algorithms are 
      random and round robin. The latter distributes requests to each server in turn. This is easy to implement and 
      usually works well.
    </para>
    <para>
      Round robin load balancing can break down when there is a large variance in the time it takes to process requests.
      When some requests are fast, but others take a lot of time to handle, it may happen that one server gets a
      disproportial number of "difficult" requests to process. These requests then queue up on the unlucky server and
      spend most of their time waiting to be served. 
    </para>
    <para>
      This can be prevented by making the load balancer more intelligent. For instance, it could measure the time it 
      takes a server to process a request and send requests to the server that responds fastest. This algorithm is 
      known as least response time, and it gives overloaded servers a chance to catch up.
    </para>
    <para>
      There are other load balancing algorithms, like weigted round robin and least connections. Watch your
      servers for relative response times and utilization (see <xref linkend="logging"/>) to find out if you're 
      affected by request distribution problems and need to switch algorithms. You can easilly do that by reconfiguring 
      your load balancer.
    </para>
  </section>
  
  <section>
    <title>Summary</title>
    <para>
      This chapter deals with two important aspects of REST APIs: performance and scalability. These qualities are
      important from both the client and server perspective. For the client, performance is one aspect of usability. For
      the server, performance and scalability may have dramatic economic consequences.
    </para>
    <para>
      Another quality that is important for both client and server is security, to which we turn our attention next.
    </para>
  </section>
</chapter>
