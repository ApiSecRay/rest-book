<chapter id="test">
  <title>Testing</title>
  <para>
    Now that you've implemented your API, you'll want to make sure that it works as expected. The most efficient way
    known today of doing that (when done right) is by testing. We define <firstterm>testing</firstterm> as checking
    against expectations.
  </para>
  <para>
    There are many different ways to test. Don’t exclusively rely on only one type of test, but reduce your overall
    risks by combining several types of tests. A classification of tests using the Ws may help with determining what
    types of tests are useful for your situation: 
  </para>
  <variablelist>
    <varlistentry>
      <term>Who</term>
      <listitem>
        <para>
          The first question to ask is whose expectations we are checking. There are at least two answers to that
          question: customers and developers, and so the Agile community talks about <firstterm>programmer 
          tests</firstterm> and <firstterm>customer tests</firstterm>.
        </para>
        <para>
          As every writer knows, the audience shapes the form his message must take. So we'll write programmer tests
          in a programming language and customer tests in a form that is accessible for non-developers, like Gherkin
          (see <xref linkend="bdd"/>).
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term>What</term>
      <listitem>
        <para>
          In <xref linkend="non-functional-requirements"/>), we discussed the many qualities of software that matter in 
          addition to functionality. For each of those important qualities, you will want to check that your API
          implementation matches the expectations. So you'll have functional testing, performance testing, scalability
          testing, security testing, etc.
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term>When</term>
      <listitem>
        <para>
          Tests can be written after the code is finished to <emphasis>verify</emphasis> that it works, or 
          they can be written first to <emphasis>specify</emphasis> how the code should work. 
          Writing the test first may seem counter-intuitive or unnatural, but there are some advantages to this
          approach: it guarantees that the code will be testable, it's more efficient to prevent bugs than to introduce
          and then fix them, and tests can be used to drive the design and implementation.
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term>Where</term>
      <listitem>
        <para>
          Tests can be written at different levels of abstraction. Unit tests test a single unit (e.g. class) in
          isolation. Integration tests focus on how the units work together. System tests look at the application as a
          whole. As you move up the abstraction level from unit to system, you require fewer tests
          <citation>Test Pyramid</citation>. You may want to go even further and test a collection of applications that
          work together, for instance in a microservices architecture.
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term>Why</term>
      <listitem>
        <para>
          There can be different reasons for writing tests. All tests verify that the code works as expected, but we've
          also seen that tests can serve as specifications of how yet-to-be-written code should work. In the latter
          situation, the tests exists not only for verification, but also to facilitate communication about how the
          application should behave, as we've seen.
          Tests can also be written to drive the code and its design. This is called <firstterm>Test-Driven
          Design</firstterm> (TDD).
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term>How</term>
      <listitem>
        <para>
          Tests can be performed by a human or by a computer program. Manual testing is most useful in the form of
          <firstterm>exploratory testing</firstterm>, where a skilled tester uses his domain knowledge to explore all
          corners of the system. Most other forms of testing should be automated as much as possible.
        </para>
        <para>
          The amount of software you ship will usually grow over time as you add features, and your testing effort will
          do so as well. If you don’t automate your tests, you testing effort will grow until it starts getting in the
          way of making progress. Automation can keep the costs of testing down.
        </para>
        <para>
          Another aspect of <emphasis>how</emphasis> is whether the tests look into the system under test or whether
          they only use the public API. The former is called <firstterm>white-box</firstterm> testing; the latter is
          <firstterm>black-box</firstterm> testing.
        </para>
      </listitem>
    </varlistentry>
  </variablelist>
  <para>
    As even this short treatment shows, testing is a vast topic. We can only scratch the surface in this book, so we
    will focus only on those aspects of testing that are specific to testing APIs.
  </para>
  
  <section>
    <title>Functional Testing</title>
    <para>
      You will want to make sure your API implementation works as expected. You will also want to make sure that
      no change you introduce later will break existing functionality. And you most likely will want to start small
      and grow your system over time. Maybe you even want to iterate quickly based on feedback from your API's users.
      To meet all those requirements, you need to automate your tests.
    </para>
    <para>
      We hope you already have unit tests in place to white-box test your implementation. We also hope you have a
      Continuous Integration (CI) server in place, which runs the unit tests on every commit. The next step is to
      create a suite of automated black-box tests against your API.
    </para>
    <para>
      While you certainly <emphasis>can</emphasis> write white-box tests at the API level, we suggest you write
      black-box tests instead (or in addition to). The ultimate test of your system is that your clients can use the
      API, and they won't be able to see your system's internals. If your API isn't sufficient to make black-box testing
      possible, chances are that it isn't complete enough for some clients to do their work.
    </para>
    <para>
      If you started your analysis using Behavior-Driven Development (BDD), as we suggested in <xref linkend="analysis"/>,
      then you're already halfway to an automated black-box test suite. All you need to do is write some glue code that
      translates the BDD scenarios into HTTP calls against your server and that verifies the assertions made in the
      <literal>Then</literal> parts of the scenarios.
    </para>
    <para>
      There are many BDD frameworks, depending on your programming environment. We only have room to show one here, and 
      we'll use Java and the JBehave framework <citation>JBehave</citation>. Other frameworks offer comparable features, 
      so even if your not using JBehave, or even Java, the following should still give you a good idea of how to proceed.
    </para>
    <para>
      If you don't have BDD scenarios at all, then you'll need some other method of creating test specifications. The 
      details of what follows will be different, but the idea remains the same: make calls into your API to verify that
      the implementation matches expectations. 
    </para>
    <para>
      JBehave makes it relatively easy to translate Gherkin scenarios into Java calls. For each line of the scenario,
      you must create a Java method that gets executed when the scenario parser processes the line. Let's see how that
      works using <xref linkend="bdd-customer-happy-path"/>.
    </para>
    <para>
      The first line is <literal>Given a customer Chrissy</literal>. Remember that each line starts with a keyword,
      in this case <literal>Given</literal>. JBehave has an annotation for each keyword, which helps the scenario
      parser map the line to a Java method using the text inside the annotation: 
    </para>
    <programlisting>public class RestbuckSteps {
    
  private String customer;

  @Given("a customer $customer")
  public void setCustomer(String customer) {
    this.customer = customer;
  }
  
}</programlisting>
    <para>
      The mapping from scenario line to Java method uses a sort of regular expression pattern matching, which enables
      mapping variable parts of the scenario line onto method parameters. In the example above, that is the case
      for the name of the customer. We either need that information in the method itself, or we save it for later use,
      for instance when we're verifying expectations.
    </para>
    <para>
      Parameterizing methods like this means we can write another scenario with a line like <literal>Given a customer
      Joe</literal>, and we wouldn't need a new method to match it. As you add more scenarios, you'll find that most
      lines in them can already be matched by existing methods, so your testing effort <emphasis>decreases</emphasis>
      over time, rather than increase.
    </para>
    <para>
      The next line is <literal>When she reads the menu</literal>, for which we add a method using the
      <literal>When</literal> annotation:
    </para>
    <programlisting>@When("she reads the menu")
public void getMenu() {
}</programlisting>
    <para>
      TODO: Finish
    </para>
  </section>

  <section id="sdk">
    <title>Building an SDK</title>
    <para>
      The test code you write is just that: code. Therefore the same considerations apply to it as to the code that
      implements your API. You'll need to evolve your tests over time just like you need to evolve your implementation, 
      and it pays to treat your test code with the same care.
      In particular, keep your test code as DRY (see <xref linkend="dry"/>) as your implementation code. That will make 
      it much less painful to change your tests later.
    </para>
    <para>
      Keeping your test code DRY has an interesting side effect. Since the tests exercise your API, they are in fact a 
      client of it. So with DRY test code you can't help but build up a <firstterm>Software Development Kit</firstterm>
      (SDK) for your API. Remember from <xref linkend="dx"/> that offering an SDK is a good way to improve the DX.
    </para>
    <para>
      Since your tests play the role of client of your API, they should follow the guidelines for clients that we
      present in <xref linkend="client"/>
    </para>
    <para>
      For a good and quick feedback cycle, you will want to keep your Continuous Integration jobs fast. So if you make
      the API tests part of your Continuous Integration pipeline, then you'll face pressure to keep the test times down.
      This pressure is your friend when it comes to building an SDK, because it will encourage you to introduce caching 
      and other performance tricks (see <xref linkend="performance"/>). Your SDK users will automatically benefit from
      this effort.
    </para>
    <para>
      The SDK you're building up when implementing your tests will be written in whatever language you have chosen for
      your automating your tests. If some clients are not written in that language, you may have to spend some 
      additional effort to port your SDK to the languages that your API users (i.e. the developers that build clients) 
      want to use. This is additional work, but you have an existing SDK that you can start from. And while you're
      porting your SDK, developers can look at the current SDK to see how to call your API.
    </para>
    <para>
      The SDK powers your functional tests, and hopefully delights your users. But it can do more. As discussed earlier,
      you will have different kinds of tests in addition to functional tests, like performance tests. These tests can
      obviously benefit from the SDK as well.
    </para>
    <para>
      An SDK can make using an API easier by hiding implementation details, like the specifics of a certain HTTP client
      library. The client developer then doesn't need to learn how to use a HTTP library; he can simply make method
      calls. You have to be careful with this, however, so that you don't fall for any of the 8 Fallacies of Distributed
      Computing <citation>Rotem06</citation>. Especially the first two (the network is reliable, latency is zero) will
      get you into trouble.
    </para>
    <para>
      Hiding implementation details can shield a client from breaking changes (see <xref linkend="breaking-changes"/>).
      Maybe you exchanged an extension link relation for a newly standardized one. The SDK can check for either and use
      the one that the server provides. This code can be hidden in the SDK's implementation, so that clients using the 
      SDK will never see any difference and don't need to be changed.
    </para>
    <para>
      SDKs can also lower the barrier to entry for your API. Package your SDK up in the native artifact format for the
      target programming language, like a .NET assembly or a Java jar. Then upload the package to a place where the
      standard package managers can find them, like NuGet or Maven Central. This allows client developers to get 
      started quickly, using idioms they already know. The faster they can get their <literal>Hello, world!</literal>
      equivalent running, the less likely are they to give up in frustration and abandon your API.
    </para>
    <para>
      At the time of writing, RADL has no tooling to generate SDKs, but it's not hard to conceive of such help.
      A big part of the SDK consists of following link relations and (de)serializing programming language objects 
      into HTTP messages. The API description contains all the information necessary to perform these tasks, so it's
      just a matter of putting the work in to build the tooling.
    </para>
  </section>
  
  <section id="test-security">
    <title>Security Testing</title>
    <para>test for each identified threat</para>
    <para>static and dynamic analysis, fuzzing</para>
  </section>
  
  <section>
    <title>Summary</title>
  </section>
</chapter>
