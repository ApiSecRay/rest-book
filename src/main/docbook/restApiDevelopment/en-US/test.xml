<chapter id="test">
  <title>Testing</title>
  <para>
    Now that you've implemented your API, you'll want to make sure that it works as expected. The most efficient way
    known today of doing that (when done right) is by testing. We define <firstterm>testing</firstterm> as checking
    against expectations.
  </para>
  <para>
    There are many different ways to test. Don’t exclusively rely on only one type of test, but reduce your overall
    risks by combining several types of tests. A classification of tests using the Ws may help with determining what
    types of tests are useful for your situation: 
  </para>
  <variablelist>
    <varlistentry>
      <term>Who</term>
      <listitem>
        <para>
          The first question to ask is whose expectations we are checking. There are at least two answers to that
          question: customers and developers, and so the Agile community talks about <firstterm>programmer 
          tests</firstterm> and <firstterm>customer tests</firstterm>.
        </para>
        <para>
          As every writer knows, the audience shapes the form his message must take. So we'll write programmer tests
          in a programming language and customer tests in a form that is accessible for non-developers, like Gherkin
          (see <xref linkend="bdd"/>).
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term>What</term>
      <listitem>
        <para>
          In <xref linkend="non-functional-requirements"/>), we discussed the many qualities of software that matter in 
          addition to functionality. For each of those important qualities, you will want to check that your API
          implementation matches the expectations. So you'll have functional testing, performance testing, scalability
          testing, security testing, etc.
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term>When</term>
      <listitem>
        <para>
          Tests can be written after the code is finished to <emphasis>verify</emphasis> that it works, or 
          they can be written first to <emphasis>specify</emphasis> how the code should work. 
          Writing the test first may seem counter-intuitive or unnatural, but there are some advantages to this
          approach: it guarantees that the code will be testable, it's more efficient to prevent bugs than to introduce
          and then fix them, and tests can be used to drive the design and implementation.
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term>Where</term>
      <listitem>
        <para>
          Tests can be written at different levels of abstraction. Unit tests test a single unit (e.g. class) in
          isolation. Integration tests focus on how the units work together. System tests look at the application as a
          whole. As you move up the abstraction level from unit to system, you require fewer tests
          <citation>Test Pyramid</citation>. You may want to go even further and test a collection of applications that
          work together, for instance in a microservices architecture.
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term>Why</term>
      <listitem>
        <para>
          There can be different reasons for writing tests. All tests verify that the code works as expected, but we've
          also seen that tests can serve as specifications of how yet-to-be-written code should work. In the latter
          situation, the tests exists not only for verification, but also to facilitate communication about how the
          application should behave, as we've seen.
          Tests can also be written to drive the code and its design. This is called <firstterm>Test-Driven
          Design</firstterm> (TDD).
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term>How</term>
      <listitem>
        <para>
          Tests can be performed by a human or by a computer program. Manual testing is most useful in the form of
          <firstterm>exploratory testing</firstterm>, where a skilled tester uses his domain knowledge to explore all
          corners of the system. Most other forms of testing should be automated as much as possible.
        </para>
        <para>
          The amount of software you ship will usually grow over time as you add features, and your testing effort will
          do so as well. If you don’t automate your tests, you testing effort will grow until it starts getting in the
          way of making progress. Automation can keep the costs of testing down.
        </para>
        <para>
          Another aspect of <emphasis>how</emphasis> is whether the tests look into the system under test or whether
          they only use the public API. The former is called <firstterm>white-box</firstterm> testing; the latter is
          <firstterm>black-box</firstterm> testing.
        </para>
      </listitem>
    </varlistentry>
  </variablelist>
  <para>
    As even this short treatment shows, testing is a vast topic. We can only scratch the surface in this book, so we
    will focus only on those aspects of testing that are specific to testing APIs.
  </para>
  
  <section>
    <title>Functional Testing</title>
    <para>
      You will want to make sure your API implementation works as expected. You will also want to make sure that
      no change you introduce later will break existing functionality. And you most likely will want to start small
      and grow your system over time. Maybe you even want to iterate quickly based on feedback from your API's users.
      To meet all those requirements, you need to automate your tests.
    </para>
    <para>
      We hope you already have unit tests in place to white-box test your implementation. We also hope you have a
      Continuous Integration (CI) server in place, which runs the unit tests on every commit. The next step is to
      create a suite of automated black-box tests against your API.
    </para>
    <para>
      While you certainly <emphasis>can</emphasis> write white-box tests at the API level, we suggest you write
      black-box tests instead (or in addition to). The ultimate test of your system is that your clients can use the
      API, and they won't be able to see your system's internals. If your API isn't sufficient to make black-box testing
      possible, chances are that it isn't complete enough for some clients to do their work.
    </para>
    <para>
      If you started your analysis using Behavior-Driven Development (BDD), as we suggested in <xref linkend="analysis"/>,
      then you're already halfway to an automated black-box test suite. All you need to do is write some glue code that
      translates the BDD scenarios into HTTP calls against your server and that verifies the assertions made in the
      <literal>Then</literal> parts of the scenarios.
    </para>
    <para>
      There are many BDD frameworks, depending on your programming environment. We only have room to show one here, and 
      we'll use Java and the JBehave framework <citation>JBehave</citation>. Other frameworks offer comparable features, 
      so even if your not using JBehave, or even Java, the following should still give you a good idea of how to proceed.
    </para>
    <para>
      If you don't have BDD scenarios at all, then you'll need some other method of creating test specifications. The 
      details of what follows will be different, but the idea remains the same: make calls into your API to verify that
      the implementation matches expectations. 
    </para>
    <para>
      It's very useful for debugging purposes to use a correlation ID (see <xref linkend="correlation"/>) so that the
      different requests and responses for a given test case can be related. This is otherwise non-trivial when you
      run tests in parallel (as you should to get quicker feedback). The test case name makes a good candidate as the
      value of the correlation ID.
    </para>
    <para>
      JBehave makes it relatively easy to translate Gherkin scenarios into Java calls. For each line of the scenario,
      you must create a Java method that gets executed when the scenario parser processes the line. Let's see how that
      works using <xref linkend="bdd-customer-happy-path"/>.
    </para>
    <para>
      The first line is <literal>Given a customer Chrissy</literal>. Remember that each line starts with a keyword,
      in this case <literal>Given</literal>. JBehave has an annotation for each keyword, which helps the scenario
      parser map the line to a Java method using the text inside the annotation: 
    </para>
    <programlisting>public class RestbuckSteps {
    
  private String customer;

  @Given("a customer $customer")
  public void setCustomer(String customer) {
    this.customer = customer;
  }
  
}</programlisting>
    <para>
      The mapping from scenario line to Java method uses a sort of regular expression pattern matching, which enables
      mapping variable parts of the scenario line onto method parameters. In the example above, that is the case
      for the name of the customer. We either need that information in the method itself, or we save it for later use,
      for instance when we're verifying expectations.
    </para>
    <para>
      Parameterizing methods like this means we can write another scenario with a line like <literal>Given a customer
      Joe</literal>, and we wouldn't need a new method to match it. As you add more scenarios, you'll find that most
      lines in them can already be matched by existing methods, so your testing effort <emphasis>decreases</emphasis>
      over time, rather than increase.
    </para>
    <para>
      The next line is <literal>When she reads the menu</literal>, for which we add a method using the
      <literal>When</literal> annotation:
    </para>
    <programlisting>@When("she reads the menu")
public void getMenu() {
}</programlisting>
    <para>
      TODO: Finish
    </para>
    <para>
      We'll talk more about testing in the face of changes in <xref linkend="consumer-contract"/>.
    </para>
  </section>

  <section id="sdk">
    <title>Building an SDK</title>
    <para>
      The test code you write is just that: code. Therefore the same considerations apply to it as to the code that
      implements your API. You'll need to evolve your tests over time just like you need to evolve your implementation, 
      and it pays to treat your test code with the same care.
      In particular, keep your test code as DRY (see <xref linkend="dry"/>) as your implementation code. That will make 
      it much less painful to change your tests later.
    </para>
    <para>
      Since the tests exercise your API, they are in fact a client of it. Keeping your test code DRY has the interesting 
      side effectof building up a <firstterm>Software Development Kit</firstterm> (SDK) for your API. Remember from 
      <xref linkend="dx"/> that offering an SDK is a good way to improve the DX.
    </para>
    <para>
      Since the tests play the role of API client, they should follow the guidelines for clients presented in 
      <xref linkend="client"/>. As you write your tests, you're effectively doing what client developers would be doing.
      This gives you a sense of what it's like to use the API. You may even decide to change the API based on this
      feedback to make it friendlier to use.
    </para>
    <para>
      For a good and quick feedback cycle, you will want to keep your Continuous Integration jobs fast. So if you make
      the API tests part of your Continuous Integration pipeline, then you'll face pressure to keep the test times down.
      This pressure is your friend when it comes to building an SDK, because it will encourage you to introduce caching 
      and other performance tricks (see <xref linkend="performance"/>). Your SDK users will automatically benefit from
      this effort.
    </para>
    <para>
      The SDK you're building up when implementing your tests will be written in whatever language you have chosen for
      your automating your tests. If some clients are not written in that language, you may have to spend some 
      additional effort to port your SDK to the languages that your API users (i.e. the developers that build clients) 
      want to use. This is additional work, but you have an existing SDK that you can start from. And while you're
      porting your SDK, developers can look at the current SDK to see how to call your API.
    </para>
    <para>
      The SDK powers your functional tests, and hopefully delights your users. But it can do more. As discussed earlier,
      you will have different kinds of tests in addition to functional tests, like performance tests. These tests can
      obviously benefit from the SDK as well.
    </para>
    <para>
      An SDK can make using an API easier by hiding implementation details, like the specifics of a certain HTTP client
      library. The client developer then doesn't need to learn how to use a HTTP library; he can simply make method
      calls. You have to be careful with this, however, so that you don't fall for any of the 8 Fallacies of Distributed
      Computing <citation>Rotem06</citation>. Especially the first two (the network is reliable, latency is zero) will
      get you into trouble.
    </para>
    <para>
      Hiding implementation details can shield a client from breaking changes (see <xref linkend="breaking-changes"/>).
      Maybe you exchanged an extension link relation for a newly standardized one. The SDK can check for either and use
      the one that the server provides. This code can be hidden in the SDK's implementation, so that clients using the 
      SDK will never see any difference and don't need to be changed.
    </para>
    <para>
      SDKs can also lower the barrier to entry for your API. Package your SDK up in the native artifact format for the
      target programming language, like a .NET assembly or a Java jar. Then upload the package to a place where the
      standard package managers can find them, like NuGet or Maven Central. This allows client developers to get 
      started quickly, using idioms they already know. The faster they can get their <literal>Hello, world!</literal>
      equivalent running, the less likely are they to give up in frustration and abandon your API.
    </para>
    <para>
      At the time of writing, RADL has no tooling to generate SDKs, but it's not hard to conceive of such help.
      A big part of the SDK consists of following link relations and (de)serializing programming language objects 
      into HTTP messages. The API description contains all the information necessary to perform these tasks, so it's
      just a matter of putting the work in to build the tooling.
    </para>
    <para>
      You should put the SDKs on GitHub or an equivalent source code repository so they're easily found by client
      developers and easy for you to update. As a bonus, this invites client developers to submit bug fixes, or even
      ports of the SDK into a programming language that you don't currently support.
    </para>
  </section>
  
  <section id="test-security">
    <title>Security Testing</title>
    <para>
      You will want to verify that your system meets the expectations with regard to security. Remember from
      <xref linkend="security"/> that security problems split roughly evenly between design flaws and implementation
      bugs. The first part of security testing deals with design flaws. This is really just testing the system's
      security features, using the techniques for functional testing described above.
    </para>
    <para>
      Remember that we derived the system's security features during threat modeling.
      For example, <xref linkend="security-features"/> indicates that we want to use TLS to protect against spoofing
      the server. We should write a test that checks whether connecting to our server over HTTPS works. This test should
      verify the TLS protocol, ciphers, and the server's certificate. We should write another test that verifies that
      we can't connect to the server using plain HTTP.
    </para>
    <para>
      This is a common pattern for functional security tests: you write a positive test that verifies that the security
      feature is in place, but also a negative test that proves you can't get around the protection.
    </para>
    <para>
      Some tests may be difficult to perform, like tests for DoS protection. It may help to configure your server to
      facilitate testing. For instance, suppose that in production, we allow our order queue size to be at most 100.
      It may be difficult or expensive to reach those numbers in a test, so we should make the limit configurable and 
      use a much lower number in our test environment than we would in production.
    </para>
    <para>
      Other security features may even be impossible to verify in a test environment. Maybe you're using a Hardware
      Security Module (HSM) in production, for example, for managing and safeguarding cryptographic keys. 
      You may not have budget to purchase another one just for testing purposes. Luckily, the features that are 
      most expensive to replicate in a test environment are usually also least likely to change. So you can manually 
      verify that they work before you launch and then, say, annually to verify they continue to do so.
    </para>
    <para>
      Some features may be to difficult for you to test because you lack the required security skills. You can opt to 
      outsource this testing to an external company that is specialized in these matters. This is known as a 
      <firstterm>penetration test</firstterm> or pentest.
      Obviously you can't do this in an automated fashion on every commit, so this shouldn't be your only option.
    </para>
    <para>
      Penetration testers are not cheap, but well worth the money. In some cases, they are virtually a must. For 
      instance, as a cloud service provider, you probably don't want every customers to go into your data center to
      perform an audit. You can still satisfy the customer's curiosity for your security posture by handing them a
      report from a reputable pentesting company.
    </para>
    <para>
      Penetration testers can work with a black-box kind of approach, where they don't have much insight into your
      defenses, or you can provide them with as much detail as they want. The former gives them a level playing field
      with real-world attackers, but it may limit what they can find. Therefore, we favor the white-box approach where
      the pentesters have access to architectural diagrams, the threat model, etc. Some companies even give pentesters
      access to source code.
    </para>
    <para>
      With access to source code, a pentester can perform <firstterm>Static Analysis Security Testing</firstterm> (SAST).
      This involves feeding the source code into a special program that understands the programming language and
      libraries used and can find security problems. These problems are usually implementation bugs rather than
      design flaws.
    </para>
    <para>
      There are also SAST tools that can look at compiled binaries rather than source code. This allows you to scan
      any third party code that you embed in addition to your own code. Verifying external code is part of a larger
      <firstterm>supply chain management</firstterm> effort that is becoming ever more important for securing systems.
      It is estimated that roughly 70% of vulnerabilities are now caused by third party code. This is not surprising,
      given our increased reliance on open source.
    </para>
    <para>
      SAST tools are good at finding certain types of bugs that are very relevant for testing APIs.
      <firstterm>Injection attacks</firstterm> happen when the client submits data through the API that can be 
      interpreted as code.
      The most (in)famous example of this is SQL Injection, where data from the client is appended to an SQL query.
      This data is intended to be just data, but through the use of special characters like quotes the data can be
      interpreted as code instead.
    </para>
    <para>
      We discussed the tools to withstand these types of attacks in <xref linkend="output-encoding"/> and
      <xref linkend="input-validation"/>. The trick is to judiciously apply them everywhere, because an attacker needs
      only one hole to get in. SAST tools can help you find the places where you've missed something.
    </para>
    <para>
      You don't need a pentester to operate a SAST tool, of course. While some of these tools may have somewhat of a 
      learning curve, any good developer can master them. The next step is to integrate the SAST tool into your
      Continuous Integration pipeline. These tools can take a long time to run, so you may not want to do so on every
      commit, but rather once per day. 
    </para>
    <para>
      SAST tools do their magic without running the application. In contrast, <firstterm>Dynamic Application Security
      Testing</firstterm> (DAST) works against a running instance of your software. Like SAST, DAST finds mostly
      implementation bugs, but the types of bugs found are somewhat different. It therefore pays to perform both
      kinds of tests.
    </para>
    <para>
      DAST tools can either exercise your API directly, or they can sit as a proxy in between the client and server.
      The latter setup makes a lot of sense for integrating them into your Continuous Integration pipeline. An example
      of an open source tool that works well in such a configuration is Zed Attack Proxy <citation>ZAP</citation>.
    </para>
    <para>
      An interesting approach to security testing is <firstterm>BDD Security</firstterm>, which couples BDD with
      security tools like ZAP <citation>BDD Security</citation>. It thus unifies functional and security testing through
      a single technique and toolchain.
    </para>
    <para>
      BDD Security also comes with a set of predefined scenarios to test various security features that are common
      across applications. This is a positive application of the notion of attack library we saw in
      <xref linkend="tm"/>.
    </para>
    <para>
      One special type of DAST tool may be interesting for testing APIs. <firstterm>Fuzz testing</firstterm>
      or <firstterm>fuzzing</firstterm> involves providing invalid, unexpected, or random data as input. This data is
      usually generated from valid input. The program is then monitored for exceptions such as crashes, or other
      unwanted behavior. This is a good way to verify the completeness of your input validation. Fuzz testing often 
      finds odd oversights and defects which human testers would fail to find, and even careful human test designers
      would fail to create tests for.
    </para>
  </section>
  
  <section>
    <title>Summary</title>
    <para>
      This chapter briefly explores how to test the API implementation for functional and security problems. After
      establishing that the implementation meets our expectations, we can deploy it into production and start running
      the service. This is the topic of the next chapter.
    </para>
  </section>
</chapter>
